# @package _global_

#class UNetModel(nn.Module):
#    """
#    The full UNet model with attention and timestep embedding.
#    :param in_channels: channels in the input Tensor.
#    :param model_channels: base channel count for the model.
#    :param out_channels: channels in the output Tensor.
#    :param num_res_blocks: number of residual blocks per downsample.
#    :param attention_resolutions: a collection of downsample rates at which
#        attention will take place. May be a set, list, or tuple.
#        For example, if this contains 4, then at 4x downsampling, attention
#        will be used.
#    :param dropout: the dropout probability.
#    :param channel_mult: channel multiplier for each level of the UNet.
#    :param conv_resample: if True, use learned convolutions for upsampling and
#        downsampling.
#    :param dims: determines if the signal is 1D, 2D, or 3D.
#    :param num_classes: if specified (as an int), then this model will be
#        class-conditional with `num_classes` classes.
#    :param use_checkpoint: use gradient checkpointing to reduce memory usage.
#    :param num_heads: the number of attention heads in each attention layer.
#    :param num_heads_channels: if specified, ignore num_heads and instead use
#                               a fixed channel width per attention head.
#    :param num_heads_upsample: works with num_heads to set a different number
#                               of heads for upsampling. Deprecated.
#    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.
#    :param resblock_updown: use residual blocks for up/downsampling.
#    :param use_new_attention_order: use a different attention pattern for potentially
#                                    increased efficiency.
#    """
#
#    in_channels: int
#    model_channels: int = 128
#    out_channels: int = 3
#    num_res_blocks: int = 2
#    attention_resolutions: Tuple[int, ...] = (1, 2, 2, 2)
#    dropout: float = 0.0
#    channel_mult: Tuple[int, ...] = (1, 2, 4, 8)
#    conv_resample: bool = True
#    dims: int = 2
#    num_classes: Optional[int] = None
#    use_checkpoint: bool = False
#    num_heads: int = 1
#    num_head_channels: int = -1
#    num_heads_upsample: int = -1
#    use_scale_shift_norm: bool = False
#    resblock_updown: bool = False
#    use_new_attention_order: bool = False
#    with_fourier_features: bool = False
#    ignore_time: bool = False
#    input_projection: bool = True
#
#    image_size: int = -1  # not used...
#    _target_: str = "lib.models.gd_unet.UNetModel"


    #"cifar10": {
    #    "in_channels": 3,
    #    "model_channels": 128,
    #    "out_channels": 3,
    #    "num_res_blocks": 4,
    #    "attention_resolutions": [2],
    #    "dropout": 0.3,
    #    "channel_mult": [2, 2, 2],
    #    "conv_resample": False,
    #    "dims": 2,
    #    "num_classes": None,
    #    "use_checkpoint": False,
    #    "num_heads": 1,
    #    "num_head_channels": -1,
    #    "num_heads_upsample": -1,
    #    "use_scale_shift_norm": True,
    #    "resblock_updown": False,
    #    "use_new_attention_order": True,
    #    "with_fourier_features": False,
    #},

model:
  _target_: cdnp.model.flow_matching.flow_matching.FlowMatching
  backbone:
    _target_: cdnp.model.meta.unet.UNetModel
    in_channels: ${data.in_channels}
    out_channels: ${data.in_channels}
    model_channels: 128
    num_res_blocks: 4
    attention_resolutions: [2]
    dropout: 0.3
    channel_mult: [2, 2, 2]
    conv_resample: false
    dims: 2
    num_classes: ${data.num_classes}
    use_checkpoint: false
    num_heads: 1
    num_head_channels: -1
    num_heads_upsample: -1
    use_scale_shift_norm: true
    resblock_updown: false
    use_new_attention_order: true
    with_fourier_features: false
  skewed_timesteps: true
  edm_schedule: true
  ode_method: "heun2"
  ode_opts:
    atol: 1e-5
    rtol: 1e-5
    nfe: 50
    step_size: null
  num_channels: ${data.in_channels}
  height: ${data.height}
  width: ${data.width}

execution:
  ema_rate: 0.999