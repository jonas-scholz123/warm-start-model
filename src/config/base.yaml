defaults:
  - base_cfg
  - _self_
  - mode: ???
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

hydra:
  mode: RUN
  output_subdir: null
  run:
    dir: .

paths:
  root: ${runtime.root}
  data: ${runtime.root}/_data
  raw_data: ${runtime.root}/_data/_raw
  output: ${runtime.root}/_output
  weights: ${runtime.root}/_weights
  normalisation: ${runtime.root}/_normalisation

execution:
  train_steps: ???
  seed: 42
  gradient_clip_norm: 1.0
  start_from: LATEST
  start_weights: null
  ema_rate: 0.0
  # Effective batch size is this * trainloader.batch_size
  accumulate_steps: 1
  resume: null

output:
  save_checkpoints: true
  out_dir: ${runtime.root}/_output
  wandb_project: cdnp
  log_gradients: false
  gradient_log_freq: 100
  save_freq: 5000
  eval_freq: 5000
  plot_freq: 5000
  use_tqdm: true
  log_level: INFO
  metrics: 
    - _target_: cdnp.evaluate.LossMetric
  final_metrics: []

rng:
  generator:
    _target_: torch.Generator
    device: ${runtime.device}

  cpu_generator:
    _target_: torch.Generator
    device: cpu

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4

scheduler: null

data:
  cache: false
  trainloader:
    _target_: torch.utils.data.DataLoader
    _partial_: true
    batch_size: 32
    shuffle: true
    num_workers: 8
    prefetch_factor: 2
    pin_memory: true
    persistent_workers: true
    drop_last: true
  testloader:
    _target_: torch.utils.data.DataLoader
    _partial_: true
    batch_size: 32
    shuffle: false
    num_workers: 8
    prefetch_factor: 2
    pin_memory: true
    persistent_workers: true
    drop_last: true